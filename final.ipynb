{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMAP FA20 Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ui5eQuo1DMEI",
        "QHgmmT2XDXSu"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPrJ8HLyC-Cd"
      },
      "source": [
        "# Keep this code cell here\n",
        "# Project title will be used by the reviewer\n",
        "PROJECT_TITLE = \"Mushrooms and Neural Nets\"\n",
        "NOTEBOOK_ID   = \"1-XgVe42DgWelGgEFWjdbOFg59ZEyLifN\"\n",
        "VERSION = \"FA20.10.10.2020\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui5eQuo1DMEI"
      },
      "source": [
        "\n",
        "---\n",
        "# Project Introduction\n",
        "\n",
        "<img align=\"left\" src=\"http://drive.google.com/uc?export=view&id=10GhY0AgAZLiXrJIoTj85Coaqss6ot3RN\" width=\"300\"/>\n",
        "\n",
        "• One of the biggest critiques of Machine Learning is that it is a black box. This doesn't just refer to the model's being largely unintelligible to human readers, but also to the fact that many of the algorithms and methods tend to be portrayed as only accessible to people with a PhD in mathematics.\n",
        "\n",
        "• There are many frameworks which improve the accessibility of Machine Learning: frameworks like `PyTorch`, `tensorflow`, `keras`, and `sklearn` are both hugely popular and powerful. While these frameworks are important, especially in a commercial setting, it adds further mystique to the already confusing field.\n",
        "\n",
        "• My goal will be to build a Neural Net from scratch. Often when this is done, people just have a series of NumPy equations in a 20 line hardcoded program which is not only difficult to understand, but also far from generic. I intend to create a Neural Net from scratch which is built as a series of higher level components which both show the underlying logic while also being more configurable and generic.\n",
        "\n",
        "• As a demonstration, I will use those components to build and train a Neural Net on the [mushroom dataset from UCI](https://archive.ics.uci.edu/ml/datasets/Mushroom) to demystify how Neural Nets work and how to use them for classification.\n",
        "\n",
        "<!-- • Even formulas when needed: \n",
        "$$e^x = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}$$ -->\n",
        "\n",
        "<!-- \n",
        "\n",
        "   VIDEO INSTRUCTIONS\n",
        "\n",
        "1. upload to google drive, get the share URL\n",
        "https://drive.google.com/file/d/1yGvY5a0KAqnOKf5kLh5EbbbRY4_LonAX\n",
        "\n",
        "2. convert to export URL:\n",
        "http://drive.google.com/uc?export=download&id=1yGvY5a0KAqnOKf5kLh5EbbbRY4_LonAX\n",
        "\n",
        "3. OR use some other service to host your video:\n",
        "https://storage.googleapis.com/uicourse/videos/dmap/Exact%20Instructions%20Challenge%20-%20THIS%20is%20why%20my%20kids%20hate%20me.%20%20Josh%20Darnit.mp4\n",
        "\n",
        "replace the src=\"YOUR VIDEO URL\" in the <source> tag in the next cell below\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQo5zhkeEuXL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "db625721-2b6a-42eb-9a9c-83df4e426a30"
      },
      "source": [
        "%%html\n",
        "<!-- this should be the ONLY html cell in the notebook: use markdown -->\n",
        "<div style=\"font-size:36px; max-width:800px; font-family:Times, serif;\">\n",
        " Title Your Video (and update the video URL)\n",
        "<video width=\"600\" controls>\n",
        "  <source src=\"https://drive.google.com/uc?export=download&id=1yGvY5a0KAqnOKf5kLh5EbbbRY4_LonAX\"\n",
        "  type=\"video/mp4\">\n",
        "</video>\n",
        "</div>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<!-- this should be the ONLY html cell in the notebook: use markdown -->\n",
              "<div style=\"font-size:36px; max-width:800px; font-family:Times, serif;\">\n",
              " Title Your Video (and update the video URL)\n",
              "<video width=\"600\" controls>\n",
              "  <source src=\"https://drive.google.com/uc?export=download&id=1yGvY5a0KAqnOKf5kLh5EbbbRY4_LonAX\"\n",
              "  type=\"video/mp4\">\n",
              "</video>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZK0-f54UAtE"
      },
      "source": [
        "# add your imports here for your entire project\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJvmagnZDPjW"
      },
      "source": [
        "# Data Acquisition, Selection, Cleaning\n",
        "\n",
        "## What is the dataset?\n",
        "\n",
        "The dataset contains **8,124 samples** of mushroom data, with **22 distinct attributes** collected. The goal of the dataset is to determine if a mushroom is poisonous or edible based on these attributes. A random guess offers roughly 50% accuracy, so the goal is to allow the Neural Net to train on a subset of the data, and then test its accuracy on another subset that it has not yet seen hopefully with much higher accuracy than guessing. Its ability to accurately classify data it was not trained on indicates if the model can generalize or was overfit to the training data.\n",
        "\n",
        "### Attributes and option counts\n",
        "\n",
        "Below is a list of attributes for each sample as well as the number of distinct values each attribute can take on.\n",
        "\n",
        "1. cap-shape: 6\n",
        "2. cap-surface: 4\n",
        "3. cap-color: 10\n",
        "4. bruises?: 2\n",
        "5. odor: 9\n",
        "6. gill-attachment: 4\n",
        "7. gill-spacing: 3\n",
        "8. gill-size: 2\n",
        "9. gill-color: 12\n",
        "10. stalk-shape: 2\n",
        "11. stalk-root: 7 (including \"?\")\n",
        "12. stalk-surface-above-ring: 4\n",
        "13. stalk-surface-below-ring: 4\n",
        "14. stalk-color-above-ring: 9\n",
        "15. stalk-color-below-ring: 9\n",
        "16. veil-type: 2\n",
        "17. veil-color: 4\n",
        "18. ring-number: 3\n",
        "19. ring-type: 8\n",
        "20. spore-print-color: 9\n",
        "21. population: 6\n",
        "22. habitat: 7\n",
        "\n",
        "### Cleaning and preparing the data\n",
        "\n",
        "The first thing to note is that UCI provides their data separate from the names of each attribute. They provide two files: `agaricus-lepiota.names` and `agaricus-lepiota.data`. The data file is a CSV file but without the header row. So first we will create a CSV file with the header row. I have uploaded a modified file [here](https://drive.google.com/file/d/1f6m9plNUFV_KnqHIX68Jw_RZslnra4rW/view?usp=sharing)\n",
        "\n",
        "Another thing worth noting is that attribute 11 has 2,480 missing values which are denoted by `?`\n",
        "\n",
        "Finally, the CSV has the class (poisonous or edible) stored on the first column so we should extract that into a set of labels and a set of features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHl-CFgcBq0D"
      },
      "source": [
        "## Step 1: Load the data into a dataframe\n",
        "\n",
        "Our first step is to use `pandas` to load the CSV into a dataframe. This allows us to easily manipulate the data and prepare it for our Neural Net down the road."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C5l6Vze9Cy6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "783657c2-c330-4e65-dfbe-cde120e71e6d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "MUSHROOM_URL = \"https://drive.google.com/uc?export=view&id=1f6m9plNUFV_KnqHIX68Jw_RZslnra4rW\"\n",
        "\n",
        "def install_data():\n",
        "    \"\"\"\n",
        "    Get the mushroom dataset and create a Pandas dataframe\n",
        "    \"\"\"\n",
        "    return pd.read_csv(MUSHROOM_URL)\n",
        "\n",
        "def show_mushroom_df():\n",
        "    \"\"\"\n",
        "    Show samples of the mushroom dataset\n",
        "    \"\"\"\n",
        "    mushroom_df = install_data()\n",
        "\n",
        "    return mushroom_df.head(5)\n",
        "\n",
        "show_mushroom_df()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>cap-shape</th>\n",
              "      <th>cap-surface</th>\n",
              "      <th>cap-color</th>\n",
              "      <th>bruises</th>\n",
              "      <th>odor</th>\n",
              "      <th>gill-attachment</th>\n",
              "      <th>gill-spacing</th>\n",
              "      <th>gill-size</th>\n",
              "      <th>gill-color</th>\n",
              "      <th>stalk-shape</th>\n",
              "      <th>stalk-root</th>\n",
              "      <th>stalk-surface-above-ring</th>\n",
              "      <th>stalk-surface-below-ring</th>\n",
              "      <th>stalk-color-above-ring</th>\n",
              "      <th>stalk-color-below-ring</th>\n",
              "      <th>veil-type</th>\n",
              "      <th>veil-color</th>\n",
              "      <th>ring-number</th>\n",
              "      <th>ring-type</th>\n",
              "      <th>spore-print-color</th>\n",
              "      <th>population</th>\n",
              "      <th>habitat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>p</td>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td>p</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>k</td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>y</td>\n",
              "      <td>t</td>\n",
              "      <td>a</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>k</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>l</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>n</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>p</td>\n",
              "      <td>x</td>\n",
              "      <td>y</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>p</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>g</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>w</td>\n",
              "      <td>b</td>\n",
              "      <td>k</td>\n",
              "      <td>t</td>\n",
              "      <td>e</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>e</td>\n",
              "      <td>n</td>\n",
              "      <td>a</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  class cap-shape cap-surface  ... spore-print-color population habitat\n",
              "0     p         x           s  ...                 k          s       u\n",
              "1     e         x           s  ...                 n          n       g\n",
              "2     e         b           s  ...                 n          n       m\n",
              "3     p         x           y  ...                 k          s       u\n",
              "4     e         x           s  ...                 n          a       g\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q86JsjxBvDO"
      },
      "source": [
        "## Step 2: Separate the labels and features\n",
        "\n",
        "Now that we have the dataframe, we need to separate it into two separate dataframes: one for features and one for labels. We don't want our Neural Net learning to just spit back the class label!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1dBbuxpBype",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e751efb8-d10f-40c6-8105-603bf718a72a"
      },
      "source": [
        "def separate_df(df):\n",
        "    \"\"\"\n",
        "    Separates the given dataframe into labels and features dataframes\n",
        "    \"\"\"\n",
        "    labels = df[\"class\"]\n",
        "    features = df.drop(columns=\"class\")\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "def show_features():\n",
        "    \"\"\"\n",
        "    Show samples of the features\n",
        "    \"\"\"\n",
        "    mushroom_df = install_data()\n",
        "    features, _ = separate_df(mushroom_df)\n",
        "\n",
        "    return features.head(5)\n",
        "\n",
        "# Verify features does not contain class labels\n",
        "show_features()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cap-shape</th>\n",
              "      <th>cap-surface</th>\n",
              "      <th>cap-color</th>\n",
              "      <th>bruises</th>\n",
              "      <th>odor</th>\n",
              "      <th>gill-attachment</th>\n",
              "      <th>gill-spacing</th>\n",
              "      <th>gill-size</th>\n",
              "      <th>gill-color</th>\n",
              "      <th>stalk-shape</th>\n",
              "      <th>stalk-root</th>\n",
              "      <th>stalk-surface-above-ring</th>\n",
              "      <th>stalk-surface-below-ring</th>\n",
              "      <th>stalk-color-above-ring</th>\n",
              "      <th>stalk-color-below-ring</th>\n",
              "      <th>veil-type</th>\n",
              "      <th>veil-color</th>\n",
              "      <th>ring-number</th>\n",
              "      <th>ring-type</th>\n",
              "      <th>spore-print-color</th>\n",
              "      <th>population</th>\n",
              "      <th>habitat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td>p</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>k</td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>y</td>\n",
              "      <td>t</td>\n",
              "      <td>a</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>k</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>l</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>n</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>x</td>\n",
              "      <td>y</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>p</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>x</td>\n",
              "      <td>s</td>\n",
              "      <td>g</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>w</td>\n",
              "      <td>b</td>\n",
              "      <td>k</td>\n",
              "      <td>t</td>\n",
              "      <td>e</td>\n",
              "      <td>s</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>e</td>\n",
              "      <td>n</td>\n",
              "      <td>a</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  cap-shape cap-surface cap-color  ... spore-print-color population habitat\n",
              "0         x           s         n  ...                 k          s       u\n",
              "1         x           s         y  ...                 n          n       g\n",
              "2         b           s         w  ...                 n          n       m\n",
              "3         x           y         w  ...                 k          s       u\n",
              "4         x           s         g  ...                 n          a       g\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reJYkltUB1UW"
      },
      "source": [
        "## Step 3: Encode the features\n",
        "\n",
        "Next we have to encode the labels into numbers to easily pass into the Neural Net. To do this we will use an `Encoder` from `sklearn` as we saw in an earlier lesson. This allows us to convert the data in the dataframes into `numpy` arrays of numbers. The reason we do this now instead of before separating the data was to take advantage of the easy data manipulation of `pandas` in separating the features from labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_4beJOMCE5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16436c0-68e8-4910-c620-a9bbe429d855"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encode_column(column):\n",
        "    \"\"\"\n",
        "    Encode a single column using the LabelEncoder from sklearn\n",
        "    \"\"\"\n",
        "    encoder = LabelEncoder()\n",
        "    return encoder.fit_transform(column)\n",
        "\n",
        "def encode_features(features):\n",
        "    \"\"\"\n",
        "    Encode each column of the features dataframe and return the numpy matrix\n",
        "    \"\"\"\n",
        "    encoded_features = features.copy()\n",
        "\n",
        "    for column in features.columns:\n",
        "        encoded_features[column] = encode_column(features[column])\n",
        "\n",
        "    return encoded_features.values\n",
        "\n",
        "def encode_labels(labels):\n",
        "    \"\"\"\n",
        "    Encode the labels dataframe (only one column)\n",
        "    \"\"\"\n",
        "    return encode_column(labels)\n",
        "\n",
        "def show_encoded_features():\n",
        "    \"\"\"\n",
        "    Show what the encoded features looks like in comparison to the pandas dataframe earlier\n",
        "    \"\"\"\n",
        "    mushroom_df = install_data()\n",
        "    features, _ = separate_df(mushroom_df)\n",
        "\n",
        "    print(\"Encoded features (first 5):\")\n",
        "    print(encode_features(features)[0:5])\n",
        "\n",
        "show_encoded_features()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded features (first 5):\n",
            "[[5 2 4 1 6 1 0 1 4 0 3 2 2 7 7 0 2 1 4 2 3 5]\n",
            " [5 2 9 1 0 1 0 0 4 0 2 2 2 7 7 0 2 1 4 3 2 1]\n",
            " [0 2 8 1 3 1 0 0 5 0 2 2 2 7 7 0 2 1 4 3 2 3]\n",
            " [5 3 8 1 6 1 0 1 5 0 3 2 2 7 7 0 2 1 4 2 3 5]\n",
            " [5 2 3 0 5 1 1 0 4 1 3 2 2 7 7 0 2 1 0 3 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3fDbo7hDtl5"
      },
      "source": [
        "## Step 4: Separate into subsets\n",
        "\n",
        "Finally our data is in the right format for our Neural Net. Now we just need to separate our data into 3 sets: training, validation, and test. The reason why we separate the data now rather than training on all the data is to test our network's ability to generalize what it learned and classify data it was not trained on. The purposes of each set are as follows:\n",
        "\n",
        "- Training: this is the set of features and labels that is fed into the Neural Net\n",
        "\n",
        "- Validation: this is a set of data which we use to progressively verify the accuracy of the Neural Net as it learns. This gives us an indicator of if it is working without training the entire network (which can be time consuming)\n",
        "\n",
        "- Test: this data is completely separate and only used at the end when the model is fully trained to verify the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmBTe0iNDx6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978dd66f-fe2c-495c-912b-b82b466fa78c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "VALIDATION_SIZE = 0.1 # 10% is reserved for validation\n",
        "TEST_SIZE = 0.1 / (1.0 - VALIDATION_SIZE) # 10% reserved for testing (adjusted because of remaining size)\n",
        "\n",
        "def seeded_dataset_split(features, labels, size):\n",
        "    \"\"\"\n",
        "    Separate the data with consistent seeded shuffling. This is important for consistency of results\n",
        "    \"\"\"\n",
        "    return train_test_split(features, labels, test_size=size, shuffle=True, random_state=0)\n",
        "\n",
        "def separate_datasets(features, labels):\n",
        "    \"\"\"\n",
        "    Separate the given data into 3 sets: training, validation, and testing\n",
        "    \"\"\"\n",
        "    remaining_features, val_features, remaining_labels, val_labels = seeded_dataset_split(features, labels, size=VALIDATION_SIZE)\n",
        "    train_features, test_features, train_labels, test_labels = seeded_dataset_split(remaining_features, remaining_labels, size=TEST_SIZE)\n",
        "\n",
        "    return (train_features, train_labels), (val_features, val_labels), (test_features, test_labels)\n",
        "\n",
        "def show_dataset_counts():\n",
        "    \"\"\"\n",
        "    Show the distribution of data between the different datasets\n",
        "    \"\"\"\n",
        "    mushroom_df = install_data()\n",
        "    features, labels = separate_df(mushroom_df)\n",
        "\n",
        "    encoded_features = encode_features(features)\n",
        "    encoded_labels = encode_labels(labels)\n",
        "\n",
        "    train_data, val_data, test_data = separate_datasets(encoded_features, encoded_labels)\n",
        "\n",
        "    print(\"Training samples:\", len(train_data[0]))\n",
        "    print(\"Validation samples:\", len(val_data[0]))\n",
        "    print(\"Testing samples:\", len(test_data[0]))\n",
        "\n",
        "show_dataset_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples: 6498\n",
            "Validation samples: 813\n",
            "Testing samples: 813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZiK2fxwGz80"
      },
      "source": [
        "# Data Exploration\n",
        "\n",
        "## Distribution\n",
        "\n",
        "A useful statistics for when we later measure the success of the Neural Net is to find out the baseline. Based on the data, the classes are distributed nearly equally:\n",
        "\n",
        "- 51.8% edible\n",
        "- 48.2% poisonous\n",
        "\n",
        "This means we will be looking to roughly beat 50% accuracy (random guessing) as an indication of learning (though we should be able to achieve far higher)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_jUYbJa9gwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2eb19f-93a0-455c-b717-76775e68a2b6"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "def get_accuracy(y, y_hat):\n",
        "    \"\"\"\n",
        "    Get the accuracy given the labels and the predictions\n",
        "    \"\"\"\n",
        "    return (y == y_hat).mean()\n",
        "\n",
        "def generate_random_labels(count):\n",
        "    \"\"\"\n",
        "    Generate a given amount of random labels\n",
        "    \"\"\"\n",
        "    return np.random.choice([0, 1], count)\n",
        "\n",
        "def show_random_accuracy():\n",
        "    \"\"\"\n",
        "    Show the accuracy of randomly guessing the labels as a baseline\n",
        "    \"\"\"\n",
        "    mushroom_df = install_data()\n",
        "    _, labels = separate_df(mushroom_df)\n",
        "\n",
        "    encoded_labels = encode_labels(labels)\n",
        "    random_labels = generate_random_labels(len(encoded_labels))\n",
        "\n",
        "    print(\"Actual labels:\")\n",
        "    print(encoded_labels)\n",
        "    print(\"Random labels:\")\n",
        "    print(random_labels)\n",
        "\n",
        "    print(\"Random guess accuracy:\", get_accuracy(encoded_labels, random_labels))\n",
        "\n",
        "show_random_accuracy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual labels:\n",
            "[1 0 0 ... 0 1 0]\n",
            "Random labels:\n",
            "[0 1 1 ... 0 1 1]\n",
            "Random guess accuracy: 0.4981536189069424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwZZTFzR8_pz"
      },
      "source": [
        "# Neural Nets\n",
        "\n",
        "### What is a Neural Net?\n",
        "\n",
        "A neural network, in its simplest incarnation, is a series of connected layers which consist of nodes intended to mimic the behavior of neurons. The neural network is one of the more black-box models in Machine Learning because of their sheer size and complexity. A single neural net can consist of millions of parameters with tens or hundreds of hyperparameters that can be tuned to improve results, and the math consists largely of linear algebra and calculus. The beauty of neural nets is that they are designed such that you can easily customize them and they determine which patterns in the data are worth learning rather than you needing to decide what is relevant to the classification process yourself.\n",
        "\n",
        "<img align=\"center\" src=\"http://drive.google.com/uc?export=view&id=1HDCjczqDrLM03Pyp2ONFrrj9nZEsZsI8\" width=\"400\"/>\n",
        "\n",
        "### How do Neural Nets make predictions?\n",
        "\n",
        "Neural nets are fairly simple to use once they have been constructed. The process of making a prediction is known as the `forward pass`. Given some input data with dimensions matching the network's input layer size, we can push the data through the layers consecutively until we reach the final layer which provides an output prediction.\n",
        "\n",
        "### How do Neural Nets learn from data?\n",
        "\n",
        "Once you have gotten a prediction for some given input data, you need the network to *learn* from that data. This process is known as the `backward pass` which involves `backpropagation`. Given the predicted output and the correct output, we can calculate the network's error. We then push the data back through the layers, updating the layers as we go with the goal of moving the layer's weights towards values that would be more likely to produce the desired output for the given input data.\n",
        "\n",
        "We will discuss the math and implementation of this piece by piece as we go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWX6z26V-RIq"
      },
      "source": [
        "## Neural Net Components\n",
        "\n",
        "We are going to implement a series of basic building blocks of a neural net. The goal is to build these components to be plug and play and allow for a configurable and extendable neural net. This is one of the amazing parts of libraries like `PyTorch` where it is simple to build basic Neural Nets and highly configurable if you want to build your own components and integrate them.\n",
        "\n",
        "We will implement each component along with a discussion of what that component is, how it works, and why we use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TO_YcBs9uvQ"
      },
      "source": [
        "### Neural Nets - Activation Functions\n",
        "\n",
        "Activation functions are an important part of neural nets. They serve as a transformation between layers which accepts the output of a layer and manipulates it to serve as an input to the next layer.\n",
        "\n",
        "One of the key reasons to use activation functions as opposed to directly feeding layer outputs to the next layer is to prevent data from being amplified in undesirable directions (for instance numbers becoming huge in a certain direction). Different functions have different strengths and weaknesses and can be chosen experimentally depending on the problem.\n",
        "\n",
        "Another advantage of activation functions is it increases the ability of the neural net to learn complex data by using a non-linear activation function.\n",
        "\n",
        "**Using the activation function**\n",
        "\n",
        "<img align=\"center\" src=\"http://drive.google.com/uc?export=view&id=1lNyuZyNxoCPBnYfy1QHKOHwo8__Qh_ec\" width=\"400\"/>\n",
        "\n",
        "At a high level, the math behind an activation function is very simple. We pass the output of the linear layer (which we will discuss later) into the activation function and this provides the activated output which is the input to the next layer. This kind of passing order applies to both the forward pass and the backward pass, the main difference being the direction and the fact that the backward pass uses gradients thus it is important to pick functions which are differentiable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFRHRZra9hyJ"
      },
      "source": [
        "class ActivationFunction:\n",
        "    \"\"\"\n",
        "    Activation Function base class\n",
        "    https://en.wikipedia.org/wiki/Activation_function\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, linear_out):\n",
        "        \"\"\"\n",
        "        Evaluate the activation function on the output of the linear layer\n",
        "        \"\"\"\n",
        "        raise Exception(\"Unimplemented\")\n",
        "    \n",
        "    @classmethod\n",
        "    def backward(cls, grad_out, activated_out):\n",
        "        \"\"\"\n",
        "        Evaluate the gradient of the activation function based on the outputs from the forward pass\n",
        "        \"\"\"\n",
        "        raise Exception(\"Unimplemented\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-Aww49_Bwi"
      },
      "source": [
        "#### ReLU\n",
        "\n",
        "ReLU is an extremely popular activation function for many reasons. The first obvious advantage of ReLU is how mathematically simple it is to understand and compute. The function is:\n",
        "\n",
        "> $\n",
        "y =\n",
        "\\left\n",
        "    \\{\n",
        "        \\begin{array}{ll}\n",
        "            x & x > 0 \\\\\n",
        "            0 & x <= 0 \\\\\n",
        "        \\end{array} \n",
        "\\right.\n",
        "$\n",
        "\n",
        "This satisfies the non-linearity requirement and has a very simple gradient:\n",
        "\n",
        "> $\n",
        "\\nabla y =\n",
        "\\left\n",
        "    \\{\n",
        "        \\begin{array}{ll}\n",
        "            1 & x > 0 \\\\\n",
        "            0 & x <= 0 \\\\\n",
        "        \\end{array} \n",
        "\\right.\n",
        "$\n",
        "\n",
        "This makes our calculations simpler and our network faster, but there is also a surprising benefit of ReLU that has helped it attain such success in the field and that is that ReLU layers combined can readily approximate other non-linear functions and can do so via simpler computations. It also does not suffer from a vanishing gradient which means that the gradient doesn't approach 0 as data gets increasingly high.\n",
        "\n",
        "It is important to not underestimate the cost of complex activation functions. Training networks can be extremely time intensive and since the process of tuning networks is often experimental, long training can mean it is harder to adapt your network or even use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR9OdPFP_DjR"
      },
      "source": [
        "class ReLU(ActivationFunction):\n",
        "    \"\"\"\n",
        "    Implements the ReLU activation function (the de facto choice for linear neural nets)\n",
        "    https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, linear_out):\n",
        "        \"\"\"\n",
        "        Output the linear layer's value if it is positive, and 0 if it is not\n",
        "        \"\"\"\n",
        "        return np.maximum(linear_out, 0)\n",
        "    \n",
        "    @classmethod\n",
        "    def backward(cls, grad_out, activated_out):\n",
        "        \"\"\"\n",
        "        Clears the gradient wherever the activated_out was 0\n",
        "        \"\"\"\n",
        "        new_grad = grad_out.copy()\n",
        "        new_grad[activated_out == 0] = 0\n",
        "        return new_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnT7lFMc_DyB"
      },
      "source": [
        "#### Softmax\n",
        "\n",
        "Softmax is a more complex activation function than ReLU, but offers some distinct advantages. It can be very useful as the final activation function because it produces probabilistic outputs which are easy to understand. However, it suffers from numerical issues like underflow and overflow (where the numbers either become too small or too large to remain precise due to the limitations of the data type). The other advantage of Softmax is that its derivative combines nicely with the Cross Entropy loss function (discussed later) which is a popular choice, making the combination very common.\n",
        "\n",
        "In comparison to ReLU, the computations are far more complex (and thus can slow your model):\n",
        "\n",
        "> $ \\sigma (X) = \\frac{e^{X_i}}{\\sum_{j=1}^K e^{X_j}} \\text{ for } i=1,...,K \\text{ and } X = (X_1,...,X_K) \\in \\mathbb{R}^K $\n",
        "\n",
        "The more common version to use is a \"stable\" softmax which subtracts the max value like so:\n",
        "\n",
        "> $ \\sigma_\\text{stable} (X) = \\sigma (X - \\text{max}(X)) $\n",
        "\n",
        "This prevents overflow, though underflow can still be an issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv___WTn_EMO"
      },
      "source": [
        "class Softmax(ActivationFunction):\n",
        "    \"\"\"\n",
        "    Implements the Softmax activation function\n",
        "    https://en.wikipedia.org/wiki/Softmax_function\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, linear_out):\n",
        "        \"\"\"\n",
        "        Compute a stabilized softmax to avoid overflow\n",
        "        \"\"\"\n",
        "        normalized_out = linear_out - np.max(linear_out, axis=1).reshape(len(linear_out), 1)\n",
        "        amplitude = np.exp(normalized_out)\n",
        "        norm = np.sum(amplitude, axis=1).reshape(len(linear_out), 1)\n",
        "        return amplitude / norm\n",
        "    \n",
        "    @classmethod\n",
        "    def backward(cls, grad_out, activated_out):\n",
        "        \"\"\"\n",
        "        The gradient of softmax does not need to be computed independently as it combines\n",
        "        nicely with the cross entropy loss function. Softmax will not be used in between\n",
        "        layers and is instead used to promote smooth outputs\n",
        "        \"\"\"\n",
        "        raise Exception(\"Unimplemented\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjnfxWjJ9n-M"
      },
      "source": [
        "### Neural Nets - Layers\n",
        "\n",
        "Neural nets consist of a series of layers which are connected to each other. Each layer contains weights which are continually adjusted to improve the ability of the neural net to represent the data it is fed. There are many kinds of layers as well as ways to connect them, but we will focus on linear layers that are fully connected.\n",
        "\n",
        "**Initializing the layer**\n",
        "\n",
        "The layer contains a weight matrix and a bias vector. They are initialized to small random values to prevent nodes from learning the exact same information because they start at slightly different values.\n",
        "\n",
        "**Configuring the layer**\n",
        "\n",
        "Each layer has an input size and an output size. The output size of a layer matches the input size of the layer that follows it. The first layer is important because it matches the size of the feature data, and the final layer matches the output classes (though there are ways to use different output sizes to represent classes than a 1-to-1 correlation). The layers in between the input and output layers are called hidden layers and offer the most room for configuration. Adding more nodes for instance can increase the layer's ability to contain different information on the data but can also cause the layer to overfit to the input data.\n",
        "\n",
        "**Computing the forward pass**\n",
        "\n",
        "This means that every node in a layer is connected with every node in the next layer. These linear layers follow a simple equation for the forward pass:\n",
        "\n",
        "> $ y = W \\cdot x + b $\n",
        "\n",
        "Finally the output of that linear transformation is fed through the activation function.\n",
        "\n",
        "> $ y' = f(W \\cdot x + b)$\n",
        "\n",
        "**Computing the backward pass**\n",
        "\n",
        "As we compute the backward pass of the neural net, we update the weights of each layer using a simple equation based on the gradient of the weights and the learning rate:\n",
        "\n",
        "> $ W = W - lr * \\nabla W $\n",
        "\n",
        "These gradients can be manipulated using different optimizer strategies but the most common optimizer, SGD, uses the above simple equation without modification to move the weights towards a loss minima.\n",
        "\n",
        "Therefore we can structure the backward pass as an `optimizer step` where the optimizer will provide the distance (and direction) to move the weights and the layer will simply shift its weights in that direction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHcvNT7u99XI"
      },
      "source": [
        "class LinearLayer:\n",
        "    \"\"\"\n",
        "    A layer class which allows each layer to be configured to find dimensions\n",
        "    that are fit for the problem. Having more nodes generally increases how much\n",
        "    a network can learn but also can slow the training\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_func=ReLU):\n",
        "        \"\"\"\n",
        "        Initialize the layer for the given input and output sizes\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_func = activation_func\n",
        "\n",
        "        # Persist the layer's forward pass for the backward pass\n",
        "        self.linear_in = np.zeros(input_size)\n",
        "        self.linear_out = np.zeros(input_size)\n",
        "        self.activated_out = np.zeros(output_size)\n",
        "\n",
        "        # Initialize the layer's weights to sufficiently small random values\n",
        "        # This prevents each node from learning the same thing\n",
        "        self.W = np.random.randn(input_size, output_size) / input_size\n",
        "        self.b = np.zeros(output_size)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute the forward pass of the layer using its existing weights and store\n",
        "        the values for use in the backward pass\n",
        "        \"\"\"\n",
        "        self.linear_in = X\n",
        "        self.linear_out = X.dot(self.W) + self.b\n",
        "        self.activated_out = self.activation_func.forward(self.linear_out)\n",
        "\n",
        "        return self.activated_out\n",
        "    \n",
        "    def update(self, step_W, step_b):\n",
        "        \"\"\"\n",
        "        Update the layer's weights based on the gradient and learning rate. Decaying\n",
        "        the learning rate over time can improve the stability of the model while\n",
        "        allowing it to learn quickly initially\n",
        "        \"\"\"\n",
        "        self.W += step_W\n",
        "        self.b += step_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn4Hffcl-FR0"
      },
      "source": [
        "### Neural Nets - Loss Functions\n",
        "\n",
        "Loss functions serve two main purposes. First it acts as a way to measure how far the predictions were from being \"ideal\". It is possible for the loss to go down without the accuracy going up because the loss measures the difference between the two values, not necessarily if the prediction was rounded to be correct. Second we use the gradient of the loss function to determine how to adjust the network in the backward pass, it serves as a starting place for the gradient calculation. The goal is to shift the weights towards values that are likely to decrease that loss which, if the network is built well, means the accuracy likely improves in hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNv6TRl8-Hh-"
      },
      "source": [
        "class LossFunction:\n",
        "    \"\"\"\n",
        "    Loss Function base class\n",
        "    https://en.wikipedia.org/wiki/Loss_function\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, y, y_hat):\n",
        "        \"\"\"\n",
        "        Compute the loss between the actual labels and the predicted labels\n",
        "        \"\"\"\n",
        "        raise Exception(\"Unimplemented\")\n",
        "    \n",
        "    @classmethod\n",
        "    def backward(cls, y, y_hat):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss between the actual labels and the\n",
        "        predicted labels\n",
        "        \"\"\"\n",
        "        raise Exception(\"Unimplemented\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "191R1ect_LPG"
      },
      "source": [
        "#### Cross Entropy Loss\n",
        "\n",
        "The Cross Entropy loss function is a very popular loss function especially when combined with the softmax as the output activation function. The Cross Entropy loss function expects values between 0 and 1 which is what the softmax is designed to produce. The further the predicted value is from the actual label, the higher the loss is.\n",
        "\n",
        "The equation for the Cross Entropy loss is:\n",
        "\n",
        "> $ \\text{loss} = - \\sum y_\\text{actual} * \\text{log}(y_\\text{pred}) $\n",
        "\n",
        "An important thing to note about our implementation of the Cross Entropy gradient is that the Cross Entropy gradient and the Softmax gradient combine to simplify into a much easier calculation.\n",
        "\n",
        "> $ \\nabla \\text{loss}_\\text{CE,softmax} = y_\\text{pred} - y_\\text{actual} $\n",
        "\n",
        "This simplified calculation not only helps avoid vastly more complicated calculations (making the network even faster), but also reduces the risk of numerical error. Note that we have opted to also average these values against the data size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKvm8lIW_LgW"
      },
      "source": [
        "class CrossEntropy(LossFunction):\n",
        "    \"\"\"\n",
        "    Implements the Cross Entropy loss function\n",
        "    https://en.wikipedia.org/wiki/Cross_entropy\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, y, y_hat):\n",
        "        \"\"\"\n",
        "        Compute the Cross Entropy loss which compares softmax outputs against\n",
        "        one hot encoded labels and returns the negative sum of the products\n",
        "        \"\"\"\n",
        "        return - np.sum(y * np.ma.log(y_hat).filled(0))\n",
        "\n",
        "    @classmethod\n",
        "    def backward(cls, y, y_hat):\n",
        "        \"\"\"\n",
        "        Compute the Cross Entropy and Softmax combined gradient (due to the\n",
        "        simplification of the calculation) which subtracts the sum of the scores\n",
        "        from the predicted labels in the location of the correct label.\n",
        "        \"\"\"\n",
        "        sum_of_outputs = np.sum(y_hat, axis=1)\n",
        "        predicted_outputs = y_hat[range(len(y)), y]\n",
        "\n",
        "        d_layer = y_hat\n",
        "        d_layer[range(len(y)), y] = (predicted_outputs - sum_of_outputs) / sum_of_outputs\n",
        "        d_layer /= len(y)\n",
        "        return d_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpYYAsQH-Hw7"
      },
      "source": [
        "### Neural Nets - Optimizers\n",
        "\n",
        "The purpose an optimizer serves is to determine how best to interpret the results of the backward pass and update the layers accordingly. There are a couple common types, namely `Stochastic Gradient Descent (SGD)` and `Adam`. We will demonstrate how simple it is to build the `SGD` optimizer but also provide a base optimizer class which allows the user to supply their own custom optimizer for whatever strategy they prefer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNaNZlBy-Jxx"
      },
      "source": [
        "class Optimizer:\n",
        "    \"\"\"\n",
        "    Optimizer base class that allows for different optimization methods to be\n",
        "    plugged into the neural net\n",
        "    \"\"\"\n",
        "    \n",
        "    def step(self, layers, d_weights, d_biases, lr):\n",
        "        \"\"\"\n",
        "        Update the layer weights with the optimizer's update step algorithm\n",
        "        \"\"\"\n",
        "        raise Exception(\"Unimplemented\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFM1yJaz_PRr"
      },
      "source": [
        "#### SGD\n",
        "\n",
        "SGD is the simplest optimizer to understand as well as the most common one to use, not just in neural nets. Not only does it often perform quite well, but it is very easy to understand. It implements the equation we discussed above for how to update layers generically:\n",
        "\n",
        "> $ W = W - lr * \\nabla W $\n",
        "\n",
        "This equation is the SGD equation (while also the basis of the math used for most layer update methods like Adam). All we do is move the weights in the opposite direction of the gradient with the goal of finding a minima. A learning rate that is too large can cause the weights to overshoot the minima while one that is too small can cause the weights to get stuck at a local minima without being able to find the global minima.\n",
        "\n",
        "<img align=\"center\" src=\"http://drive.google.com/uc?export=view&id=1tZrCwflxz13hHhW28WfoEuQZP7geptyZ\" width=\"400\"/>\n",
        "\n",
        "The reason why we supply the learning rate each time is so that we can decay the learning rate as we train. This can help us avoid getting stuck in local minima but an important part of learning rate decay comes down to both the ability to train quicker as well as with more stability.\n",
        "\n",
        "If we only train with a high learning rate, we can bounce back and forth around the minima without improving the fit, so while the odds of getting closer to the global minima can be higher (since it would be less prone to getting stuck in a local minima), the model will struggle to improve performance beyond a threshold. Once you have gotten \"close enough\" to the minima, it becomes important to use a small learning rate to get as close to it as possible. We will demonstrate using a simple learning rate decay equation when we go to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iBLyTsM_Pj0"
      },
      "source": [
        "class SGD(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the Stochastic Gradient Descent optimizer\n",
        "    https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
        "    \"\"\"\n",
        "    \n",
        "    def step(self, layers, d_weights, d_biases, lr):\n",
        "        \"\"\"\n",
        "        The SGD update step simply shifts the weights in the opposite direction\n",
        "        of the gradient\n",
        "        \"\"\"\n",
        "        for layer, d_W, d_b in zip(layers, d_weights, d_biases):\n",
        "            layer.update(-lr * d_W, -lr * d_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Lm7wSk-KMJ"
      },
      "source": [
        "## Bringing it all together\n",
        "\n",
        "The final component to implement is the Neural Net class itself. This component needs to keep track of the configuration information as well as orchestrate the forward and backward passes. Notice that the layers themselves do not know how to communicate with layers before or after them and only worry about how to transform input data to output data. That is because this final, but crucial, job is for the network itself (in our implementation). There are 3 main goals our neural net component will need to satisfy: the forward pass, the backward pass, and making predictions.\n",
        "\n",
        "### Forward Pass\n",
        "\n",
        "Notice that the forward pass produces continuous rather than discrete values. The forward pass ultimately produces two numbers which range from 0 to 1 and sum to 1 (due to the properties of the Softmax activation function). These represent effectively the \"pseudo-probability\" (in the sense that it looks like a probability) of each class being correct. The reason why we produce continuous values is because there is a difference between being barely right and being very right. If the network says the output values are 0 and 1, that indicates extreme certainty in which class is correct, versus 0.4 and 0.6 which only slightly leans to one side. We want our network to increase its confidence in correct scenarios upon training on data that reinforces its conclusion. Weak assertions therefore are weakened further and can ultimately flip if further data shows that the networks initial assumptions were incorrect.\n",
        "\n",
        "From a mathematical perspective, the forward pass is very simple. We can consider each layer to be a function whose output is the input to the next layer. The initial input is simply the data itself and the final output is our usable scores. This can be implemented via a reduction or a simple for loop depending on how comfortable you are with functional concepts.\n",
        "\n",
        "### Backward Pass\n",
        "\n",
        "TODO\n",
        "\n",
        "### Prediction\n",
        "\n",
        "Given how we structured our network's forward pass, and our decision to give each class its own output (rather than relying on rounding to separate one output into multiple classes), prediction becomes very straight forward and scalable to problems with more than the 2 classes we have right now. We simply run the data through the forward pass which gives us those pseudo-probabilities, and then the index of the larger number is the predicted class. So if we have the outputs [0.4, 0.6] for classes [0, 1], the network's prediction is the class at the 2nd position (or index 1).\n",
        "\n",
        "### Overfitting\n",
        "\n",
        "**Measuring overfitting**\n",
        "\n",
        "Overfitting is when the network learns to classify the training data very well but comparatively struggles with data that it was not trained on. This is measured by testing the network with the validation data and verifying that the validation accuracy does not lag too far behind the training accuracy. Overfitting can sometimes be so severe that a network is very good on training data but completely unusable on the validation and testing sets.\n",
        "\n",
        "**Reducing overfitting**\n",
        "\n",
        "One common strategy to avoid overfitting is to employ regularization. The result of regularization is that the network is encouraged to choose smaller weights rather than solely trying to minimize the loss. Minimizing the loss too aggressively can result in the network being extremely closely fit to the training data which prevents it from generalizing for unseen data.\n",
        "\n",
        "There are other strategies such as dropout which causes data to train different subsets of the nodes such that various parts of each layer have been trained on different data, but we will focus on regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8tiMMb7-n2S"
      },
      "source": [
        "class NeuralNet:\n",
        "    \"\"\"\n",
        "    The main neural net class which is configurable with a series of layers and\n",
        "    a given optimizer\n",
        "    https://en.wikipedia.org/wiki/Neural_network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, loss_func=CrossEntropy, optimizer=SGD(), norm_weights=False):\n",
        "        \"\"\"\n",
        "        Initialize the neural net with the given layers, loss function, and optimizer.\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "        self.loss_func = loss_func\n",
        "        self.optimizer = optimizer\n",
        "        self.norm_weights = norm_weights\n",
        "\n",
        "        self.input_size = layers[0].input_size\n",
        "        self.output_size = layers[-1].output_size\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute the forward pass of the neural net. This takes the given input\n",
        "        matrix containing the features of an arbitrary number of features and\n",
        "        pushed them through the layers and activation functions.\n",
        "        \"\"\"\n",
        "        y_hat = X\n",
        "\n",
        "        for layer in self.layers:\n",
        "            y_hat = layer.forward(y_hat)\n",
        "\n",
        "        return y_hat\n",
        "    \n",
        "    def backward(self, X, y, lr, reg=0.0):\n",
        "        \"\"\"\n",
        "        Compute the backward pass of the neural net. This takes the given features\n",
        "        and labels and pushes the features forward through the network and then\n",
        "        backward to update the layer weights and train the neural net.\n",
        "        \"\"\"\n",
        "        y_hat = self.forward(X)\n",
        "\n",
        "        # One hot encode the labels\n",
        "        y_one_hot = np.eye(self.output_size)[y]\n",
        "        loss = self.loss_func.forward(y_one_hot, y_hat)\n",
        "\n",
        "        # Calculate the final gradient from the loss function\n",
        "        d_layer = self.loss_func.backward(y, y_hat)\n",
        "\n",
        "        d_weights = []\n",
        "        d_biases = []\n",
        "\n",
        "        # Move through the layers from the final layer to the initial layer\n",
        "        for idx, layer in reversed(list(enumerate(self.layers))):\n",
        "            if (idx + 1) < len(self.layers):\n",
        "                # Layer is not the output layer\n",
        "                next_layer = self.layers[idx + 1]\n",
        "\n",
        "                # Calculate the gradient from the previous layer with the activation function\n",
        "                d_layer = d_layer.dot(next_layer.W.T)\n",
        "                d_layer = layer.activation_func.backward(d_layer, layer.activated_out)\n",
        "\n",
        "            # Calculate the layer's weight gradient\n",
        "            d_W = layer.linear_in.T.dot(d_layer) + 2 * reg * layer.W\n",
        "            d_b = np.sum(d_layer, axis=0)\n",
        "\n",
        "            # Track the layer weights for the update step after all have been calculated\n",
        "            d_weights.insert(0, d_W)\n",
        "            d_biases.insert(0, d_b)\n",
        "\n",
        "        # Use the optimizer to update the layers\n",
        "        self.optimizer.step(self.layers, d_weights, d_biases, lr)\n",
        "\n",
        "        # Helps reduce numerical errors from large numbers without affecting direction\n",
        "        if self.norm_weights:\n",
        "            w_norm = max(np.linalg.norm(layer.W) for layer in self.layers) / len(self.layers)\n",
        "\n",
        "            for layer in self.layers:\n",
        "                layer.W /= w_norm\n",
        "                layer.b /= w_norm\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def predict(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Predict the outputs of the given input features. If input labels are provided,\n",
        "        also measure the accuracy of the predictions.\n",
        "        \"\"\"\n",
        "        scores = self.forward(X)\n",
        "        predictions = np.argmax(scores, axis=1)\n",
        "\n",
        "        if y is None:\n",
        "            return predictions\n",
        "\n",
        "        loss = self.loss_func.forward(np.eye(self.output_size)[y], scores)\n",
        "        accuracy = get_accuracy(y, predictions)\n",
        "        \n",
        "        return predictions, loss / len(y), accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVqckiHyDUkD"
      },
      "source": [
        "# Training\n",
        "\n",
        "The Neural Net on its own is useless without the process of feeding it data and measuring its performance. This brings us to how to train the neural net. Training often is done as a series of `epochs` and `batches`.\n",
        "\n",
        "An epoch represents an entire \"unit\" of training where we run the dataset through the network. We will use many epochs to allow the network to train on each element in the set multiple times with varying learning rates to help it train quickly at first and then approach the minima and stabilize.\n",
        "\n",
        "Batches represent a chunk of the dataset. Rather than trying to feed the entire dataset through the network at once, we break the data into a series of batches and allow the network to improve on each batch. This gives the layer more opportunities to improve as well as avoiding some memory issues from having large datasets.\n",
        "\n",
        "**Learning rate decay**\n",
        "\n",
        "We mentioned the importance of learning rate when talking about SGD. This tweaking of the learning rate over time can be done in many different ways, but the formula that we will use is exponential:\n",
        "\n",
        "> $ lr_i = lr_0 * r^i $\n",
        "\n",
        "In plain language, the learning rate of the current epoch `i` is equal to the initial learning rate times the decay rate `r` to the power of the epoch number.\n",
        "\n",
        "**Training steps**\n",
        "\n",
        "1. Separate the data into batches\n",
        "2. For each epoch:\n",
        "\n",
        "    i. Measure the accuracy on the training data\n",
        "\n",
        "    ii. Train on each batch, summing the loss\n",
        "\n",
        "    iii. Measure the accuracy on the validation data\n",
        "\n",
        "3. Measure the accuracy on the test data\n",
        "\n",
        "It is important to log useful information throughout the training process to verify that the network is improving and not showing any alarming behavior. If you have a very large dataset, it is common to limit the accuracy measurements to only every x epochs rather than every epoch, but for our dataset we can measure them at every epoch and graph those results. For printing purposes, however, we will limit the output to \"interesting\" epochs to avoid printing out an overwhelming amount of data.\n",
        "\n",
        "**Network size and hyperparameters**\n",
        "\n",
        "Notice that we have chosen some default hyperparameters for the network in the train function. In general, when working on a neural net, it is crucial to pick good parameters and a lot of effort often goes into experimentally finding them. Notice also how easy we were able to build the network with our individual components, creating a 3 layer network with fully configurable hidden layer sizes, and a user can easily select (or build their own) loss function or optimizers using our base class, just as we did for Cross Entropy and SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCvMOQnq9ixQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443a536b-d99e-4e98-b711-ec162cd8b329"
      },
      "source": [
        "def create_mini_batches(data_size, batch_size):\n",
        "    \"\"\"\n",
        "    Create a series of batch slices to train on rather than training the network on\n",
        "    the entire training data all at once. If the data_size is not divisble by the\n",
        "    batch_size, some data can be left out, but generally not very substantial\n",
        "    assuming small enough batch_size.\n",
        "    \"\"\"\n",
        "    # If the data_size is not divisible by the batch_size, some can be cut off\n",
        "    num_batches = data_size // batch_size\n",
        "    batches = []\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        batches.append(slice(batch * batch_size, (batch + 1) * batch_size))\n",
        "    \n",
        "    return batches\n",
        "\n",
        "def train(net, train_data, val_data, epochs=75, batch_size=200, lr=0.1, lr_decay=0.99, reg=0.01):\n",
        "    \"\"\"\n",
        "    Train a given neural net with the given learning configuration.\n",
        "    \"\"\"\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    val_loss = []\n",
        "    val_accuracy = []\n",
        "\n",
        "    batches = create_mini_batches(len(train_data[0]), batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Get the current accuracy on training set\n",
        "        train_prediction = net.predict(*train_data)\n",
        "        train_loss.append(train_prediction[1])\n",
        "        train_accuracy.append(train_prediction[2])\n",
        "\n",
        "        # Get the accuracy on the validation set\n",
        "        val_prediction = net.predict(*val_data)\n",
        "        val_loss.append(val_prediction[1])\n",
        "        val_accuracy.append(val_prediction[2])\n",
        "\n",
        "        for batch in batches:\n",
        "            # Slice the data for this batch\n",
        "            X, y = [data[batch] for data in train_data]\n",
        "\n",
        "            # Run the backward pass and store the loss\n",
        "            net.backward(X, y, lr * lr_decay ** epoch, reg)\n",
        "        \n",
        "        # Print out \"interesting\" epochs\n",
        "        if epoch < 5 or (epoch + 1) % 5 == 0:\n",
        "            stats_string = \"Epoch: {:d} | Avg loss: {:.2f} | Train: {:.1f}% | Val: {:.1f}%\"\n",
        "            print(stats_string.format(epoch + 1, train_loss[-1], train_accuracy[-1] * 100, val_accuracy[-1] * 100))\n",
        "\n",
        "    return train_loss, train_accuracy, val_loss, val_accuracy\n",
        "\n",
        "\n",
        "def pipeline():\n",
        "    \"\"\"\n",
        "    Demonstrate the neural net's training results with a full start to finish\n",
        "    pipeline.\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    mushroom_df = install_data()\n",
        "    features, labels = separate_df(mushroom_df)\n",
        "\n",
        "    # Process the dataset into trainable data\n",
        "    encoded_features = encode_features(features)\n",
        "    encoded_labels = encode_labels(labels)\n",
        "\n",
        "    # Separate for each stage\n",
        "    train_data, val_data, test_data = separate_datasets(encoded_features, encoded_labels)\n",
        "\n",
        "    input_size = train_data[0].shape[1]\n",
        "    hidden_size = 128\n",
        "    output_size = 2\n",
        "\n",
        "    # Build the network\n",
        "    net = NeuralNet([\n",
        "                     LinearLayer(input_size, hidden_size, activation_func=ReLU),\n",
        "                     LinearLayer(hidden_size, hidden_size, activation_func=ReLU),\n",
        "                     LinearLayer(hidden_size, output_size, activation_func=Softmax)\n",
        "    ], loss_func=CrossEntropy, optimizer=SGD(), norm_weights=True)\n",
        "\n",
        "    # Train the network\n",
        "    train_results = train(net, train_data, val_data)\n",
        "\n",
        "    # Get the test accuracy\n",
        "    _, test_loss, test_accuracy = net.predict(*test_data)\n",
        "    print(\"Test average loss: {:.2f}\".format(test_loss))\n",
        "    print(\"Test accuracy: {:.1f}%\".format(test_accuracy * 100))\n",
        "\n",
        "    # Return the results for graphing\n",
        "    return (train_data, val_data, test_data), train_results\n",
        "\n",
        "train_data_sets, train_results = pipeline()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Avg loss: 0.69 | Train: 52.2% | Val: 52.2%\n",
            "Epoch: 2 | Avg loss: 0.57 | Train: 72.4% | Val: 70.1%\n",
            "Epoch: 3 | Avg loss: 0.39 | Train: 84.7% | Val: 83.9%\n",
            "Epoch: 4 | Avg loss: 0.33 | Train: 87.7% | Val: 87.1%\n",
            "Epoch: 5 | Avg loss: 0.29 | Train: 89.0% | Val: 88.3%\n",
            "Epoch: 10 | Avg loss: 0.17 | Train: 93.9% | Val: 93.5%\n",
            "Epoch: 15 | Avg loss: 0.11 | Train: 96.7% | Val: 96.7%\n",
            "Epoch: 20 | Avg loss: 0.08 | Train: 97.7% | Val: 97.4%\n",
            "Epoch: 25 | Avg loss: 0.06 | Train: 98.7% | Val: 98.4%\n",
            "Epoch: 30 | Avg loss: 0.04 | Train: 99.0% | Val: 99.0%\n",
            "Epoch: 35 | Avg loss: 0.03 | Train: 99.3% | Val: 99.4%\n",
            "Epoch: 40 | Avg loss: 0.02 | Train: 99.5% | Val: 99.6%\n",
            "Epoch: 45 | Avg loss: 0.02 | Train: 99.6% | Val: 99.6%\n",
            "Epoch: 50 | Avg loss: 0.01 | Train: 99.8% | Val: 99.6%\n",
            "Epoch: 55 | Avg loss: 0.01 | Train: 99.9% | Val: 99.8%\n",
            "Epoch: 60 | Avg loss: 0.01 | Train: 100.0% | Val: 99.9%\n",
            "Epoch: 65 | Avg loss: 0.01 | Train: 100.0% | Val: 100.0%\n",
            "Epoch: 70 | Avg loss: 0.01 | Train: 100.0% | Val: 100.0%\n",
            "Epoch: 75 | Avg loss: 0.01 | Train: 100.0% | Val: 100.0%\n",
            "Test average loss: 0.01\n",
            "Test accuracy: 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0mTZpxxeMxR"
      },
      "source": [
        "### Graphing the results\n",
        "\n",
        "Now that the network has been trained and we have kept track of the loss and accuracy at each epoch we should find the test accuracy and we can graph the values to look for potential problem areas or places to improve. For instance, if we see the validation accuracy plateau well below the training accuracy, this indicates that the model overfit to the training data and does not generalize well. Worse even, the accuracy could start to get worse after a certain point!\n",
        "\n",
        "#### Baseline\n",
        "\n",
        "To further establish the effectiveness of the neural net, we will use our baseline method of random guessing that we discussed at the beginning. To do this we will generate \"scores\" using a uniform distribution and then round those values to make the predictions. The reason we don't force the scores to be labels immediately is that we want to also get a baseline for the loss which cares about the small differences between a \"perfect\" prediction and a \"statistically correct\" prediction. For example, 0.6 does round to 1, but 0.8 is closer to 1 than 0.6 is.\n",
        "\n",
        "#### Loss\n",
        "\n",
        "The scale of the loss values matters less than the general trends. We should expect the loss to start high and decrease at a decreasing rate.\n",
        "\n",
        "#### Accuracy\n",
        "\n",
        "The accuracy should reflect a mirror image of the loss where the accuracy starts low and improves quickly but starts to flatten out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM9RE8KE_goq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "22723585-1e38-4c62-998c-92012987cfa6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def train_random(train_data, epochs=75):\n",
        "    \"\"\"\n",
        "    Create a baseline by making a random guess every epoch.\n",
        "    \"\"\"\n",
        "    loss = []\n",
        "    accuracy = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        scores = np.random.rand(len(train_data[0]))\n",
        "        predictions = np.round(scores)\n",
        "        \n",
        "        loss.append(CrossEntropy.forward(train_data[1], scores) / len(scores))\n",
        "        accuracy.append(get_accuracy(train_data[1], predictions))\n",
        "    \n",
        "    return loss, accuracy\n",
        "\n",
        "def graph(baseline_loss, baseline_accuracy, train_loss, train_accuracy, val_loss, val_accuracy):\n",
        "    \"\"\"\n",
        "    Graph the neural net against the baseline.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 10))\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[0].plot(baseline_accuracy, label=\"Baseline\", color=\"gray\")\n",
        "    axes[0].plot(train_accuracy, label=\"Neural Net (train)\")\n",
        "    axes[0].plot(val_accuracy, label=\"Neural Net (val)\")\n",
        "\n",
        "    axes[0].set_title(\"Accuracy vs Epoch\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Accuracy\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot loss\n",
        "    axes[1].plot(baseline_loss, label=\"Baseline\", color=\"gray\")\n",
        "    axes[1].plot(train_loss, label=\"Neural Net (train)\")\n",
        "    axes[1].plot(val_loss, label=\"Neural Net (val)\")\n",
        "\n",
        "    axes[1].set_title(\"Average Loss vs Epoch\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"Cross Entropy Loss\")\n",
        "    axes[1].legend()\n",
        "    \n",
        "graph(*train_random(train_data_sets[0]), *train_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHgmmT2XDXSu"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "### Analyzing the graphs\n",
        "\n",
        "As you can see from the above graphs, the neural net accuracy starts off fairly even with the random baseline. This makes sense because the neural net is initialized with random weights at each layer. Over time, the neural net improves, very rapidly at first as it goes through the SGD steps. The smoothness of the network accuracy improves over time as the learning rate becomes smaller and the network approaches the global minima for the loss. By the time we stop the training process, the neural net is capable of getting just a hair under 100% accuracy on all data sets, even the data that was set aside for validation and testing. This means that our network had enough capacity to learn all that was necessary for classification. Notice how the validation accuracy trails the training accuracy by a very small amount. This indicates that the network mostly avoids overfitting to the training data.\n",
        "\n",
        "### Final thoughts\n",
        "\n",
        "Now that the neural net components have been completed and worked successfully, it should be more clear how the various pieces fit together and what purpose they each serve. If one desired, they could create different versions of components like the loss function, activation functions, and optimizers. Neural nets can seem daunting and while there are many frameworks out there to make the creation of neural nets accessible without knowing the math or pieces behind the scenes, those frameworks can make an already confusing subject even more mystifying. Hopefully seeing a simpler framework implemented from scratch helps people understand the fundamentals behind neural nets and a glimpse of their power. Some datasets and problems are more prone to overfitting or learning issues and require more tweaking and network tricks, and sometimes a neural net just isn't the right tool for the job, but in this case we can safely say that if the neural net says the mushroom is poisonous, don't eat it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv41ElVE5d0K"
      },
      "source": [
        "\n",
        "---\n",
        "# Submission Guidelines (keep this section here)\n",
        "---\n",
        "\n",
        "\n",
        "When you are ready to submit your project, part of the submission process will be to register your notebook for reviewing.  \n",
        "\n",
        "You will also receive the links and instructions to do the peer reviews.\n",
        "\n",
        "Please review the metadata:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOUCKYCWBZDp"
      },
      "source": [
        "def get_metadata():\n",
        "  meta = {\n",
        "          \"title\": PROJECT_TITLE, # keep this as is\n",
        "          \"nb_id\": NOTEBOOK_ID,   # keep this as is\n",
        "\n",
        "          \"data\": [\"https://drive.google.com/uc?export=view&id=1f6m9plNUFV_KnqHIX68Jw_RZslnra4rW\"],\n",
        "\n",
        "          # permissions\n",
        "          # do you give the instructor the permission to copy this project\n",
        "          # and allow others to view it in the class gallery?\n",
        "          \"allow_gallery\": False,\n",
        "          \n",
        "          # if your project is made viewable to others,\n",
        "          # do you want to include your name (first/last)?\n",
        "          \"allow_name_release\": False\n",
        "          }\n",
        "  return meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_C-G52O29MM"
      },
      "source": [
        "Specific instructions will come for what to submit for the various milestones.\n",
        "\n",
        "If necessary, you can download the Python version of this notebook by using the `File->Download .py` as well as the notebook itself `File->Download .ipynb`.\n",
        "\n"
      ]
    }
  ]
}